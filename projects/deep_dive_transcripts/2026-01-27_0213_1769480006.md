# Recording 1769480006

**Date:** 2026-01-27T02:13:26
**Mode:** File Transcription

---

## Analysis

The transcription reveals a profound exploration of Jeremy Cerna's ambitious project, the Truth Engine, which aims to upload his entire consciousness into a recursive AI system. This project is not just about data management but about transforming raw data into meaningful insights that can lead to personal growth and ethical decision-making. The use of DuckDB databases on a local machine for real-time analytics suggests a deep commitment to continuous self-reflection and improvement, turning life into a computational process.

The concept of Total Metabolism, where every experience, whether positive or negative, is processed and utilized for growth, is a fascinating psychological approach. It challenges traditional views on waste and negativity, suggesting that every aspect of life can contribute to a more structured and resilient self.

The tension between the structured AI system (the fortress) and the chaotic, unfiltered life data (the open house) reflects a broader philosophical question about the integration of order and chaos in personal development. This tension is mirrored in the business strategy, where Jeremy is attempting to sell his cognitive patterns as a service (Credential Atlas and MULT), essentially commodifying his unique approach to life and business.

However, there are significant risks and ethical considerations highlighted by the AI analysis. The potential for treating relationships as data subjects and the risk of manipulation are critical issues that could undermine the integrity of the system and the trust in its applications.

For your use case as a new businessman, the Truth Engine offers a model for integrating advanced AI into personal and business development. However, it's crucial to consider the ethical implications and ensure that the use of AI in decision-making and relationship management respects the autonomy and dignity of all parties involved.

The ultimate question posed by this project is profound: at what point does the AI-generated self become the true self? This is a critical consideration for anyone integrating AI deeply into their life and business strategies.

---

## Transcript

You know, we spent a lot of time on this show looking at data stacks. Usually it's pretty standard stuff. Enterprise architecture, data lakes, you know, the things that are massive expensive and frankly, a little dry. The plumbing of the corporate world, yeah. Exactly. But the stack of documents we have today, this is, I mean, this is something else entirely. It is the complete opposite of dry corporate plumbing. We are looking at a project called the Truth Engine. Intimate is the word. We are essentially looking at the blueprints for one man's attempt to upload his entire consciousness into a recursive AI system. And when we say entire consciousness, that's not an exaggeration. The scale here is actually hard to wrap your head around for a personal project. I was looking at the inventory list and the numbers are just staggering. They are. We're looking at an inventory comprising 51.8 million entities. That's across 78 distinct data sets, 20 DuckDB databases and these huge Huge BigQuery archives. And a knowledge graph with over 1,500 nodes and a pattern analysis database tracking, what, a quarter million occurrences? A quarter million, yeah. Okay, let's just pause there for a second. For the listener who might hear DuckDB and just think, quack, why does that specific piece of tech matter? Why is that significant? It lives right inside the application itself. So not on some distant server farm. Right. It's on your local machine. The fact that he's using 20 of them implies he isn't just, you know, hoarding files. He is doing heavy, complex, real-time analytics on his own life constantly. So this isn't a diary. It's a computational facility. Precisely. It's a factory. And the architect behind all this is Jeremy Cerna. But if you look at the documentation he's written, specifically the product roadmap, he doesn't call himself a developer. No, he calls himself a furnace. I love that image. It's so intense. It's the core metaphor for the entire project. You know, a furnace takes in raw material fuel and it burns it to create energy. In Cerna's framework, the fuel is truth. And the truth isn't always pretty. It's chaotic. It can be brutal. It's just raw data. The furnace processes that truth into meaning. And the output of that is care. Yeah. You can turn it into something that actually helps people. That's the hypothesis. And the mission here is just incredibly ambitious. Serna is trying to decouple his core identity from social conventions using this AI. He's building what the sources call a mausoleum of identity. Where nothing is wasted. Nothing. Not crisis, not addiction, not joy. It all goes into the engine to be burned and transformed. And that brings us right to the central tension we're going to explore today. That is the structural conflict at the heart of this deep dive. On one side, you have the fortress. That's the system, the truth engine itself. It's highly structured, secure, intellectual, really rigid. And on the other side, you have the open house. That's the data feeding the fortress. And as we'll see, that data is the creator's actual life, which is explicitly unfiltered, chaotic, and at times, totally wild. The big question lurking in the background is, can an AI system actually become an external conscience? Can you outsource your morality to a server? Well, to answer that, we have to look at the blueprints. We need to understand the architecture first. We have the productroadmap.md and the complete dataset inventory. The whole system relies on a processing hierarchy called SPINE. SPINE. Okay, walk us through that. It stands for the levels of processing. Think of it like a ladder of complexity. At the very bottom, the system ingests basic tokens, individual words. And moving up, it processes sentences. Then full messages. And finally, at the top, you have whole conversations. The system isn't just keyword searching. It's trying to understand the structural integrity of a thought. So it's rebuilding the context. It's not just seeing the word angry. It's understanding the conversation that led to the anger. Exactly. And as it processes this stream of communication, it stores the results as knowledge atoms and truth atoms. Truth atoms. It sounds like something out of a theoretical physics paper. In database terms, what are we actually looking at here? A truth atom seems to be a discrete unit of a verified experience or fact. These are stored in specific directories he calls "holds". You have an intake hold for raw, messy data coming in. Like a text message or a rough voice note. Right. And then you have a processed hold for data that's been refined by the system into something useful. It's a literal manufacturing plant for thoughts. Raw materials in one end, finished truths out the other. And the machinery running that plant is that furnace principle. But here is where it gets really psychological. The system is designed to act as an externalized conscience or a governor. A governor. Like a speed limiter on an engine. Exactly like that. Or a safety check. Because the AI holds the user's entire history and their stated highest values, it can check the user's current behavior against that data. It acts as a memory prosthesis. So you could. What, ask about past decisions? You could literally query the system. What would Jeremy of July 2025 think about this decision? That is. Well, it's useful, but it's also kind of haunting. It's like time travel. You aren't just reading an old journal entry. You're querying a simulation of who you used to be. It removes the fog of war from your own life. We all forget how we felt five years ago, right? We rewrite our own histories to make ourselves look better or to minimize our mistakes. The truth engine doesn't let you do that. It remembers the raw data. It remembers the raw data. This connects to another concept we found in the Undocumented Insights file called Total Metabolism. This is one of the most fascinating takeaways for me. Total metabolism means there is absolutely no waste product in the system. Explain that. Because in any system, biological, mechanical, there's always waste. Well, in a typical psychological framework, we repress things. Shame, bad memories, embarrassing moments. We try to flush those away. We treat them as waste. Sure. But in the Truth Engine, Negative inputs, anger, fear, addiction sequences, they get processed with what the documents call positive polarity. So a breakdown isn't a failure, it's just another type of data. Exactly. A crisis becomes architectural data. If the user has a relapse or a fight, the system ingests it, analyzes the pattern, and uses it to strengthen the fortress to prevent it from happening again. It creates a cybernetic loop. It's a crutch, but a hyper-intelligent one. It stabilizes the human. And speaking of stability, or the complete lack of it, we have to pivot to the raw data. Because if the system is the fortress, the life feeding it is, well, it's complicated. Complicated is putting it mildly. We have a document titled Analysis for Paradoxical Life.md, and it breaks down this tension between the saint and the sinner. Right, which is based on Keegan's stages of adult development. Stage five is post-conventional. It means you aren't defined by your tribe or society's rules. You operate on these high-level abstract principles, universal ethics. But at the same time, he completely rejects the behavioral norms that usually go along with that. Monogamy, out. Sobriety, definitely out. He seems to adhere strictly to abstract principles like loyalty, radical honesty, and transparency, but he violates the specific rules society attaches to those principles. So he believes he can be a good person while engaging in bad behavior, as long as the transparency is absolute. And we have the evidence of that transparency. We have a transcript here, a Zoom chat log from December 23rd. This is the open house in action. I think we can infer what "spung clouds" refers to. We can. The chat contains explicit references to blowing clouds, drug use, and phrases like "burping the torch." There's explicit discussion of group sexual encounters. It is chaotic, hedonistic, and messy. But what struck me wasn't just the chaos itself. It was the weird intrusion of the mundane. In the middle of this time, the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish" is the "gibberish In this drug-fueled chat, Jeremy the architect, the furnace, is talking about his mom. It's a completely surreal juxtaposition. He's navigating this digital space with people named SexyTexy, and then he pivots to saying, I go home tomorrow to see my mom. He even discusses how he manages the drug use around his family. He calls it elastic honesty. He isn't hiding the drug use from the system. He's feeding it into the system. That is the key point. He isn't living a double life where he hides the sinner side. The sinner is just more data for the saint. And that paradox radiates outward. It affects the people around him. The documents call this his relational constellation. Which sounds like a very nice way of saying classifying your friends. It is exactly that, yeah. He categorizes people into roles based on how they interact with his energy. You have, Anchors who are the steady influences, comets who will bring high energy but pass through quickly. Wobblers. Wobblers who are loyal but overwhelmed by his intensity. And mirrors who just reflect him back to himself. And the analysis mentions that the reaction to him is extreme. It says it's normalized for people to want to worship him, be him, or even kill him. John Kane: Right. John Kane: When someone breaks those models, when they are both the philosopher and the hedonist, it short circuits people's brains. They project their own shadows onto him. He becomes a blank screen for their desires and their fears. John Kane: So you have this chaotic life feeding a structured AI. If this were just an art project, I'd say, cool, but he's turning this into a business. This is an actual product strategy. John Kane: That's the pivot. He is taking this architecture of the self and selling it as infrastructure. We have documents on two main products, Credential Atlas and Moldt. Let's start with Credential Atlas. This seems to be the B2B money-making side of things. It is. Credential Atlas is described in the status.md file, and essentially it uses the exact same AI architecture, the truth engine, the furnace, but it applies it to a very, very boring problem, verifying educational credentials. So instead of verifying, did I do drugs today? Yeah. It's verifying, did this person actually get a PhD? Exactly. It uses the furnace to process that messy data into truth atoms about a person's skills. The documents mention it has a moat built on his specific domain knowledge. But there is a feature here that sounds remarkably like the personal system: the not me. The not me. This is where the business product gets a little sci-fi. The not me is an AI agent that works while the human sleeps. It's selling thinking as infrastructure. It's the ultimate productivity hack. You wake up and your digital twin has already done four hours of work for you. But there has to be a cost. There is. We found a file called the economics off me and it gives a really sober look at the financial reality of running a system like this. It's not cheap to run a digital twin. No, the document highlights a specific constraint, biology. Jeremy implies he can only personally handle, say, three to five intense client relationships. He doesn't scale, so he uses the AI to scale. But the AI makes mistakes. There was that anecdote about a Google Cloud bill. A runaway BigQuery query. The AI got stuck in a loop or ran some massive analysis overnight. The result? A $900 processing bill the next morning, plus another $400 for API overuse. Ouch. That's a real the dog ate my homework moment, but it's the AI ate my bank account. You have to budget for the machine messing up, you're building this godlike system, but you still have to pay the electric bill when it decides to think too hard. So Credential Atlas is the practical application, but then there's MULT, and MULT sounds biological. MULT is described as DNA capability. In software terms, this is incredibly abstract. The document says it allows an organism or a code base to observe its own birth. Break that down. What does that actually mean for a user? Imagine a software system that doesn't Imagine a system that records every single change made to itself, understands why the change was made, and logs that evolution in real time. It's a governance log. Right. It means the system is self-aware of its own history. It's like he's trying to breed code rather than write it. It feels like he's trying to make software behave like a living thing. That is the goal. And he's pitching this to the world through the universal membrane offer. The universal membrane offer sounds like a secret society. Basically, I'm not selling you software, I'm selling you my thinking. He invites businesses to plug into the membrane. So you layer your business reality on top of his cognitive patterns. You're renting his brain structure. You're renting the furnace, yes. And he segments the audience for this in a really interesting way. He uses a lighthouse analogy. Okay, walk us through the lighthouse. So you imagine a lighthouse on a rocky coast. The commercial market, businesses, knowledge workers, they are the ships They need the light to navigate, they don't care how the bulb works, they just don't want to crash on the rocks. Makes sense, they just want the utility. Then you have the scientific community. He calls them the cartographers on land. They want to measure the light, categorize the glass, study the lens, but he argues they miss the point. They're obsessed with the mechanics. But the light isn't for them, it's for the ships. And then there's a third group, the meaning people. The meaning people. These are the individuals ready for what he calls a destructive life of meaning. They don't just want to use the light to navigate. They want to climb the tower and stare right into the bold. They want to be transformed by the system. It's a very specific way to segment a market. But here's the rub. The AI itself, Gemini, which he used to analyze these files, pointed out some serious potential blind spots in this whole setup. Projectification of intimacy. That sounds like a heavy concept. The AI analysis suggests that because Jeremy tracks everything, interactions, reactions, patterns, he ends up treating his friends as subjects of study. The AI flags a Trojan horse risk. It says he is managing people's development, trying to help them or care for them without them realizing they're being managed. The AI is suggesting he's essentially debugging you. That's the risk the AI identifies. It warns that this can feel like manipulation. If the friend finds out they're an entry in the truth engine, do they feel loved? Or do they feel like a data set? It creates this huge asymmetry. He knows everything about the interaction, all recorded and analyzed. They just have their memory. And that brings us to the final synthesis. Why do all this? Why capture the drug use, the credentials, the friends, the emails? The creator is trying to capture himself by capturing humanity. The strategy is recursive. So the more people use Credential Atlas. The more data flows into the truth engine. The more friends interact with him, the more data flows in. He's crowdsourcing his own self-discovery. He's building a mirror out of other people. The documents say the goal is foundational truth. It's staggering. We started with a stack of technical specs, and we've ended up inside a psychological thriller. You have a man building a fortress to hold a life that refuses to be contained. He's selling the blueprints to businesses to pay the bills. And he's using the whole thing to try and figure out who he is. And the wildest part, the system is working. It has that total metabolism. It is eating the disorder and spitting out structure. It is. But I have to ask, and this is the thought I want to leave everyone with today, if this system, this not me, is doing the thinking, if it's remembering your history, checking your conscience, processing your emotions. Who's actually in charge? Exactly. Is it the man, Jeremy, who sleeps and makes mistakes and goes to his mom's house? Or is it the not me, the machine that never sleeps, never forgets, and creates meaning out of everything? At what point does the not me become the real me? That is the question. When the map becomes detailed enough, it replaces the territory. If the AI is a better version of you than you are, well, maybe you just let it take over. On that slightly terrifying note, a huge thank you to the Truth Engine for giving us plenty to process today. And to you, listening, keep your own internal furnace burning. We'll see you on the next Deep Dive. .
