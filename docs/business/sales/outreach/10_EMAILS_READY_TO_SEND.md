# 10 Emails Ready to Send

**Created:** December 31, 2025
**Purpose:** Backup scheduled sends + templates for manual sending
**Schedule:** Fri Jan 3 (5), Sat Jan 4 (3), Sun Jan 5 (2)

---

## How to Schedule as Backup

**Gmail:**
1. Compose email, paste content
2. Click dropdown arrow next to Send
3. Select "Schedule send"
4. Pick date/time (8:00 AM MT recommended)

**Schedule these NOW as backup. Then send manually when ready (cancel the scheduled one).**

---

## EMAIL 1: Renaissance Learning
**Schedule:** Friday, January 3, 2026 @ 8:00 AM MT
**To:** [Find CTO or VP Data on LinkedIn] - *Target: Todd Brekhus (CPO) or Chris Dragon (COO/Former CTO)*
**Subject:** AI verification for adaptive assessment - relevant to Renaissance?

**Research Notes for Personalization:**
*   **Hook:** Mention their recent "Responsible AI" guidance (Aug 2025).
*   **Hook:** Mention the Savvas partnership (Aug 2025) - integrating assessment + instruction data creates a higher risk of "compounded hallucinations" if data isn't verified.
*   **Angle:** "With the Savvas integration, you're bridging assessment and instruction. My system verifies that the data crossing that bridge is accurate before it hits the student."

---

Hi [Name],

Quick question: How do you verify your AI-powered adaptive assessments aren't hallucinating student capabilities before acting on the recommendations?

I built a system that solves this. Multi-tier confidence scoring, provenance tracking, queryable audit trails. Not a pitch deck - actual running infrastructure in BigQuery processing 51M+ entities.

Built it in 108 days while leading data strategy at Peterson's. Now it's ready for organizations facing the same verification gap.

The hook for Renaissance: You're deploying AI that makes real-time decisions about student learning paths. What's the verification layer between AI recommendation and student impact?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/renaissance-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 2: Ellucian
**Schedule:** Friday, January 3, 2026 @ 8:30 AM MT
**To:** [Find CTO or Chief Data Officer on LinkedIn] - *Target: Michael Wulff (CPTO)*
**Subject:** Preserving institutional memory during AI migration

**Research Notes for Personalization:**
*   **Hook:** The **Anthology SIS/ERP acquisition** (Nov 2025). This is a massive data migration event.
*   **Angle:** "Migrating Anthology's assets into the Ellucian ecosystem is a massive data integrity challenge. My system ensures that as you modernize these records, the 'institutional memory' isn't lost or hallucinated by the new AI layers."

---

Hi [Name],

You're migrating institutions to cloud + AI. How do you preserve 50+ years of institutional memory during the transition - and verify the AI isn't making decisions based on hallucinated context?

I built verification infrastructure for exactly this problem. Multi-tier truth architecture that tracks provenance, scores confidence, and creates auditable trails for AI-assisted decisions.

Background: Former strategic leader at Peterson's (EdTech). Built this system processing 51M+ entities with full lineage tracking.

The Ellucian angle: Your customers are trusting you with decades of institutional data. When AI makes a recommendation about a student or process, what's the verification layer?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/ellucian-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 3: Civitas Learning
**Schedule:** Friday, January 3, 2026 @ 9:00 AM MT
**To:** [Find Chief Analytics Officer or VP Data Science on LinkedIn] - *Target: Greg Lamp (CTO)*
**Subject:** Confidence scoring for prescriptive student workflows

**Research Notes for Personalization:**
*   **Hook:** Their "AI Readiness Guide" and focus on "Not All AI Is Built for Higher Ed".
*   **Angle:** "I saw your piece on 'The Real Risk of AI is Hesitation.' My infrastructure solves the hesitation by providing the *verification layer* that makes AI safe to deploy."

---

Hi [Name],

I read your "AI Readiness Guide" and the point about hesitation being the real risk. The hesitation comes from a lack of verification.

I built the infrastructure that solves that hesitation.

My system provides the verification layer that makes AI safe to deploy:
- Multi-tier confidence scoring (know when to trust the AI)
- Provenance tracking (know where the data came from)
- Audit trails (know why the decision was made)

The Civitas angle: You want to deploy prescriptive workflows. My system gives you the confidence score to know when to automate and when to review.

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/civitas-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 4: GoGuardian
**Schedule:** Friday, January 3, 2026 @ 9:30 AM MT
**To:** [Find Chief Product Officer or VP Data Governance on LinkedIn]
**Subject:** Auditing what AI flags vs. what it misses

---

Hi [Name],

You're protecting students with AI-powered safety tools. How do you audit what the AI flagged versus what it missed - and verify the confidence level before escalating to humans?

I built verification infrastructure that creates:
- Audit trails for AI decisions (what was flagged, why, with what confidence)
- Provenance tracking (what data informed the flag?)
- Multi-tier confidence scoring (high confidence vs. needs review)

Background: Built this while leading data strategy at Peterson's. Now processing 51M+ entities with full governance.

The GoGuardian question: When your AI flags something as concerning, what's the verification layer? When it DOESN'T flag something, how do you know what was missed?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/goguardian-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 5: Clever
**Schedule:** Friday, January 3, 2026 @ 10:00 AM MT
**To:** [Find VP Data Infrastructure or CTO on LinkedIn]
**Subject:** Truth verification across 50+ integrated apps

---

Hi [Name],

You're the data highway for K-12. When districts need to verify truth across 50+ integrated applications, what's the verification layer?

I built infrastructure for exactly this:
- Multi-source truth reconciliation (which system is authoritative?)
- Provenance tracking (where did this data originate?)
- Confidence scoring (how certain are we this is accurate?)

The Clever angle: You're moving data between dozens of systems. When there's a conflict - or when AI makes a recommendation based on that data - how do you verify what's true?

Background: Former Peterson's strategic leader. Built this processing 51M+ entities with full lineage tracking.

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/clever-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 6: Amplify
**Schedule:** Saturday, January 4, 2026 @ 8:00 AM MT
**To:** [Find VP Engineering or Chief Product Officer on LinkedIn]
**Subject:** Verification layer for AI-powered curriculum decisions

---

Hi [Name],

Amplify is deploying AI across curriculum and assessment. How do you verify the AI's recommendations before they affect student learning paths?

I built verification infrastructure that provides:
- Confidence scoring before action (Tier 1 = act, Tier 3 = review)
- Provenance tracking (what informed this recommendation?)
- Audit trails (queryable decision history)

This is running infrastructure - 51M+ entities processed with full governance. Built while leading data strategy at Peterson's.

The question for Amplify: Your AI is making recommendations about what students learn. What's between "AI suggests" and "student experiences"?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/amplify-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 7: Curriculum Associates (i-Ready)
**Schedule:** Saturday, January 4, 2026 @ 8:30 AM MT
**To:** [Find CTO or VP Data on LinkedIn]
**Subject:** Confidence scoring for adaptive learning recommendations

---

Hi [Name],

i-Ready makes adaptive recommendations that shape student learning. How do you verify the AI's confidence before those recommendations become reality?

I built the verification layer:
- Multi-tier confidence scoring
- Provenance tracking (what data drove this?)
- Audit trails for AI decisions

Running infrastructure, not a concept. 51M+ entities processed. Built at Peterson's, now ready for organizations navigating the same AI verification gap.

The i-Ready question: Adaptive means the AI is constantly deciding. What's the verification layer between "AI adapts" and "student receives"?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/iready-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 8: PowerSchool
**Schedule:** Saturday, January 4, 2026 @ 9:00 AM MT
**To:** [Find VP Analytics or CTO on LinkedIn]
**Subject:** AI verification across SIS + LMS + Analytics

---

Hi [Name],

PowerSchool spans SIS, LMS, and analytics. When AI makes recommendations using data from all three, how do you verify the confidence and track provenance?

I built verification infrastructure for multi-source AI decisions:
- Cross-system truth reconciliation
- Provenance tracking (which system, which record, when?)
- Confidence scoring before action

Background: Peterson's strategic leader, built this processing 51M+ entities.

The PowerSchool angle: You have more student data than almost anyone. When AI synthesizes across your systems, what's the verification layer?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/powerschool-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 9: D2L (Brightspace)
**Schedule:** Sunday, January 5, 2026 @ 8:00 AM MT
**To:** [Find CTO or VP Product on LinkedIn]
**Subject:** Verification layer for AI-assisted learning

---

Hi [Name],

Brightspace is integrating AI across the learning experience. How do you verify AI recommendations before they affect learner outcomes?

I built verification infrastructure that provides:
- Confidence scoring (act vs. review)
- Provenance tracking (what informed this?)
- Audit trails (queryable decision history)

Running system - 51M+ entities, full governance. Built at Peterson's.

The D2L question: AI is making more decisions in Brightspace. What's between "AI recommends" and "learner experiences"?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/d2l-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## EMAIL 10: Lightcast
**Schedule:** Sunday, January 5, 2026 @ 8:30 AM MT
**To:** [Find VP Product or Chief Data Officer on LinkedIn]
**Subject:** Confidence scoring for labor market intelligence

---

Hi [Name],

Lightcast provides labor market intelligence that informs major decisions - curriculum design, career guidance, workforce planning. How do you communicate confidence levels when the underlying data has varying reliability?

I built verification infrastructure for exactly this:
- Multi-tier confidence scoring (high confidence vs. directional)
- Provenance tracking (source, freshness, methodology)
- Audit trails for AI-derived insights

Background: Peterson's strategic leader. Built this processing 51M+ entities with full lineage.

The Lightcast question: Your insights drive real decisions. What's the verification layer between "data suggests" and "institution acts"?

I've set up a private, interactive agent trained on your context to demonstrate this:
**[Start Private Demo](https://truthengine.ai/discovery/lightcast-2026)**

Jeremy Serna
jeremy.serna@gmail.com
(940) 284-1343
Credential Atlas LLC

---

## Next Steps

1. **Find the recipients** - LinkedIn search for each role
2. **Schedule all 10 as backup** - Use Gmail schedule send
3. **Send manually when ready** - Cancel scheduled, send live
4. **Tell Claude** - "Sent email #X to [person] at [company]"

---

## Schedule Summary

| # | Company | Schedule Time | Recipient (find on LinkedIn) |
|---|---------|---------------|------------------------------|
| 1 | Renaissance Learning | Fri Jan 3, 8:00 AM | CTO or VP Data |
| 2 | Ellucian | Fri Jan 3, 8:30 AM | CTO or Chief Data Officer |
| 3 | Civitas Learning | Fri Jan 3, 9:00 AM | Chief Analytics Officer |
| 4 | GoGuardian | Fri Jan 3, 9:30 AM | CPO or VP Data Governance |
| 5 | Clever | Fri Jan 3, 10:00 AM | VP Data Infrastructure or CTO |
| 6 | Amplify | Sat Jan 4, 8:00 AM | VP Engineering or CPO |
| 7 | Curriculum Associates | Sat Jan 4, 8:30 AM | CTO or VP Data |
| 8 | PowerSchool | Sat Jan 4, 9:00 AM | VP Analytics or CTO |
| 9 | D2L (Brightspace) | Sun Jan 5, 8:00 AM | CTO or VP Product |
| 10 | Lightcast | Sun Jan 5, 8:30 AM | VP Product or CDO |

---

*All emails written. Schedule as backup. Send manually. Tell Claude when done.*
